{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this code we process the csv generated from the big query i.e big_query.csv  and save the processed csv as processed_data.csv\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "filepath = os.getcwd()+\"/dataset/big_query.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = spark.read.parquet(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import inflect\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "EN = spacy.load('en_core_web_sm')\n",
    "\n",
    "stopwords_english = stopwords.words('english')\n",
    "def tokenize_text(text):\n",
    "    \"Apply tokenization using spacy to docstrings.\"\n",
    "    tokens = EN.tokenizer(text)\n",
    "    return [token.text.lower() for token in tokens if not token.is_space]\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords_english:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def tokenize_code(text):\n",
    "    \"A very basic procedure for tokenizing code strings.\"\n",
    "    return RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return (' '.join(normalize(tokenize_text(text))).split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "def pre_process(x):                   #remove the code section\n",
    "    #updating questions i.e removing all the html tags using parsers\n",
    "    #print(x)\n",
    "    soup = BeautifulSoup(x, 'lxml')\n",
    "    if soup.code: soup.code.decompose()     # Remove the code section\n",
    "    tag_p = soup.p\n",
    "    tag_pre = soup.pre\n",
    "    text = ''\n",
    "    if tag_p: text = text + tag_p.get_text()\n",
    "    if tag_pre: text = text + tag_pre.get_text()\n",
    "    #print(tag_pre,tag_p)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def TextBlob_1(x):\n",
    "    return TextBlob(x).polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#print(data_frame.body)\n",
    "udf_myFunction = udf(pre_process, StringType())\n",
    "#print((data_frame.schema.names))\n",
    "\n",
    "\n",
    "#removing all the html tags from body and answers and titles and forming new columns for the same\n",
    "data_frame_procc = data_frame.withColumn('processed_body',udf_myFunction(data_frame.body))\n",
    "\n",
    "data_frame_procc_1 = data_frame_procc.withColumn('processed_title',udf_myFunction(data_frame.title))\n",
    "data_frame_final = data_frame_procc_1.withColumn('processed_answers',udf_myFunction(data_frame.answers))\n",
    "data_frame_final = data_frame_final.withColumn('sentiment',udf(TextBlob_1, FloatType())(data_frame_final.answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "\n",
    "#concatenating title,body, answers into joined_data\n",
    "df_new_col = data_frame_final.withColumn('joined_data', \n",
    "                    sf.concat(sf.col('title'),sf.lit(' '), sf.col('processed_body'),sf.lit(' '),sf.col('processed_answers')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now preprocessing the joined_data and tokenizing them and normalizing the score\n",
    "data_frame_tokenized = df_new_col.withColumn('joined_data',udf(preprocess_text, ArrayType(StringType()))(df_new_col.joined_data))\n",
    "min_score = data_frame_tokenized.select(\"score\").rdd.min()[0]\n",
    "max_score = data_frame_tokenized.select(\"score\").rdd.max()[0]\n",
    "mean_score = data_frame_tokenized.groupBy().avg(\"score\").take(1)[0][0]\n",
    "\n",
    "#normalizing the score\n",
    "data_frame_toknorm = data_frame_tokenized.withColumn(\"score\",(data_frame_tokenized.score-mean_score)/(max_score-min_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the processed dataframe into a parquet file\n",
    "\n",
    "save_filepath = os.getcwd()+\"/dataset/processed_data.parquet\"\n",
    "data_frame_toknorm.write.format('parquet').mode(\"overwrite\").save(save_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
